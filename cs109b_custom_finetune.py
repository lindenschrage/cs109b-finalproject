# -*- coding: utf-8 -*-
"""cs109b-custom-finetune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zVptlpL8HBiAbKxjHxKeaGOCTZcUx2lu
"""

#!pip install peft
#!pip install trl

import transformers
from transformers import LlamaModel, LlamaTokenizer
import numpy as np
import pandas as pd
import tensorflow as tf
import keras
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.utils.data as data_utils
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split
from torch.optim import Adam
from torch.nn import MSELoss
from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from huggingface_hub import snapshot_download
from transformers import LlamaModel, LlamaConfig
from transformers import PreTrainedModel, PretrainedConfig
import torch.nn as nn

from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from trl import SFTTrainer
from datasets import Dataset

access_token = "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"

tokenizer = LlamaTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')
tokenizer.pad_token = tokenizer.eos_token

url = '/n/home09/lschrage/projects/cs109b/cs109b-finalproject/dataframe.csv'
df = pd.read_csv(url)
df.head()

import torch.nn as nn
import torch
from transformers.file_utils import ModelOutput
from dataclasses import dataclass
import torch

class TweetOptimismConfig(PretrainedConfig):
    model_type = "tweet_optimism"

    def __init__(self, hidden_size=768, **kwargs):
        super().__init__(**kwargs)
        self.hidden_size = hidden_size


class TweetOptimismRegressor(PreTrainedModel):
    config_class = TweetOptimismConfig

    def __init__(self, config):
        super(TweetOptimismRegressor, self).__init__(config)
        self.llama = LlamaModel.from_pretrained("meta-llama/Llama-2-7b-hf", output_hidden_states=True, token=access_token)
        self.linear = nn.Linear(config.hidden_size, 100)
        self.regressor = nn.Linear(100, 1)
        self.loss_fn = nn.MSELoss() if not hasattr(config, 'loss_fn') else config.loss_fn
    def forward(self, input_ids, attention_mask=None, labels=None):
        if input_ids is None:
            raise ValueError("input_ids must be provided")

        kwargs = {'input_ids': input_ids}
        if attention_mask is not None:
            kwargs['attention_mask'] = attention_mask

        llama_output = self.llama(**kwargs).hidden_states[-1].mean(dim=1)
        x = self.linear(llama_output)
        logits = self.regressor(x).squeeze()

        if labels is not None:
            loss = self.loss_fn(logits, labels)
            return {"loss": loss, "logits": logits}

        return {"logits": logits}
'''
    def forward(self, input_ids, attention_mask=None, labels=None):
      if input_ids is None:
          raise ValueError("input_ids must be provided")

      llama_output = self.llama(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, -1, :]
      x = self.linear(llama_output)
      logits = self.regressor(x).squeeze()

      if labels is not None:
          loss = self.loss_fn(logits, labels)
          return {"loss": loss, "logits": logits}

      return {"logits": logits}
'''
def tokenize_texts(text_list):
    return tokenizer(text_list, max_length=300,padding=True, truncation=True, return_tensors="pt")

df['Tweet-tokens'] = df['Tweet'].apply(tokenize_texts)

X = df['Tweet-tokens'].apply(lambda x: x['input_ids'].squeeze().tolist())
y = df['TweetAvgAnnotation']

X_train_full, X_test, y_true_train_full, y_true_test = train_test_split(X, y, test_size=0.2, random_state=109)
X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_true_train_full, test_size=0.5, random_state=109)

df_train = pd.DataFrame({
    "input_ids": X_train,
    "labels": y_train
})

df_val = pd.DataFrame({
    "input_ids": X_val,
    "labels": y_val
})

train_dataset = Dataset.from_pandas(df_train)
val_dataset = Dataset.from_pandas(df_val)


if "__index_level_0__" in train_dataset.column_names:
    train_dataset = train_dataset.remove_columns(["__index_level_0__"])

if "__index_level_0__" in val_dataset.column_names:
    val_dataset = val_dataset.remove_columns(["__index_level_0__"])

config = TweetOptimismConfig(hidden_size=4096)
model = TweetOptimismRegressor(config=config)

peft_parameters = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=8,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=[
        'q_proj',
        'k_proj',
        'v_proj',
        'o_proj',
        'linear'
    ],
    modules_to_save=[
        'regressor'
    ]
)

train_params = TrainingArguments(
    output_dir="./results_modified",
    num_train_epochs=1,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    optim="paged_adamw_32bit",
    save_steps=25,
    logging_steps=25,
    learning_rate=2e-4,
    weight_decay=0.001,
    fp16=False,
    bf16=False,
    max_grad_norm=0.3,
    max_steps=-1,
    warmup_ratio=0.03,
    group_by_length=True,
    lr_scheduler_type="constant"
)


fine_tuning = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    peft_config=peft_parameters,
    dataset_text_field="text",
    tokenizer=tokenizer,
    args=train_params
)

fine_tuning.train()

#fine_tuning.model.save_pretrained('/content/drive/My Drive/cs109b-finalproject/custom_finetuned_model')
